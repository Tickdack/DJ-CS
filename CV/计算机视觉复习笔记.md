# 计算机视觉复习笔记

## 第一节——相机和预备

### 1.1 相机

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9nc3MzLmJkc3RhdGljLmNvbS8tUG8zZFNhZ194STRraEdrcG9XSzFIRjZoaHkvYmFpa2UvYzAlM0RiYWlrZTgwJTJDNSUyQzUlMkM4MCUyQzI2L3NpZ249NDY1ZThhOTI1M2RhODFjYjVhZWI4YjlmMzMwZmJiNzMvZDZjYTdiY2IwYTQ2ZjIxZjU1MzkzYzM1ZjQyNDZiNjAwYzMzYWU0My5qcGc)

* 物距---被拍摄物体到凸透镜的距离。

* 像距---成像平面到凸透镜的距离。

* 焦点---通过凸透镜的、平行主光轴的光线，在主光轴上的会聚点。

* 焦距---凸透镜中心到焦点的距离。

* 焦距固定的是定焦镜头，焦距可以调节的是变焦镜头。

* 焦距、物距、像距最基本的关系可以用高斯成像公式表示：

  $$
  {1 \over u} + {1 \over v} = {1 \over f}
  $$


* 因此，当物距为无穷远时，像距等于焦距，成像在焦平面上。

* 当物距在无穷远和两倍焦距之间时，像距在焦距和两倍焦距之间，成倒立缩小的实像（照相机）。

* 当物距等于两倍焦距时，像距等于物距，成像为与物体倒立等大的实像。

* 当物距小于两倍焦距大于焦距时，像距大于两倍焦距，成倒立放大的实像（幻灯机和部分支持微距拍摄的照相机）。

* 当物距等于焦距时，像距无穷大，光线通过透镜成为平行光线，不成像。

* 当物距小于焦距时，像距为负，在物体的同侧成正立放大的虚像（放大镜）。
	

	
	​	理论上，只有处于镜头焦点距离的景物成像是清晰的，在焦点的前后，光线开始聚集和扩散，成像变得模糊，成像点形成的是一个扩大的圆，这个圆称为弥散圆（circle of confusion）。由于人眼的分辨能力有限，只有当弥散圆直径大到一定程度时，才会感觉到模糊。这一段人眼看起来清晰的距离就是景深（depth of field），此时对应的是容许弥散圆。
	
	​	在焦点前后各有一个容许弥散圆，这两个弥散圆之间的距离是焦深，对应到被拍摄物体的前后就是景深。
	
	​	**在焦深/景深之间的图像/物体就是清晰的，在这之外的物体则是模糊的~**

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9nc3MxLmJkc3RhdGljLmNvbS85dm8zZFNhZ194STRraEdrcG9XSzFIRjZoaHkvYmFpa2UvYzAlM0RiYWlrZTgwJTJDNSUyQzUlMkM4MCUyQzI2L3NpZ249ZTZkMDhkOGExZjMwZTkyNGRiYTk5NDYzMmQ2MTA1NjMvYjgzODliNTA0ZmMyZDU2MjY0ZWQ0MDZmZTcxMTkwZWY3N2M2NmM0Zi5qcGc)

​		**在物体前后景深内的物体可以认为成像都是清晰的。**

​		焦深计算：$焦深 = {f \over D}$

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706152849655.png" alt="image-20210706152849655" style="zoom:50%;" />

​	在RGB色彩空间内，彩图是由R（红色），G（绿色），B（蓝色）叠加而成的，对于色深为256的图，每种色彩范围均在[0, 255]内，单独拎出来都是黑白图，合在一起加上*颜色深度*的属性后变为了彩图

这里补充一下RGB转灰度图的方法：常用的即为RGB三通道取均值法与心理学公式法：
$$
gray=0.299∗R+0.578*G+0.114∗B
$$
<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706153932202.png" alt="image-20210706153932202" style="zoom:50%;" />

### 1.2 拜尔滤镜：（这应该不是重点）

一种将RGB滤色器排列在光传感组件方格之上所形成的马赛克彩色滤色阵列。

这种滤色器的排列[注 2]有50%是绿色，25%是红色，另外25%是蓝色，因此也称做**RGBG。**

> **绿色光传感器称作光敏侦测组件，而红、蓝色则称为色敏侦测组件。使用两倍于红色或蓝色的绿色组件来模仿人眼的生理性质。
> **
>
> **人类视网膜白天同时使用了M与L视锥细胞来感光，对绿光最敏感。拜尔滤色镜相机的原始图像文件称作拜尔图像影像。**
>
> **因为每个像素只过滤并记录RGB三种颜色的一种，这些从单个像素获取的信息并不能完整表现红、绿、蓝各色的组成数值。为了得到全色彩影像，可用不同的去马赛克算法来插值得到每个像素的红、绿、蓝色的组成数值。**
>
> ![查看源图像](https://www.stuff-review.com/wp-content/uploads/2012/03/bayer-filter-array-on-sensor-1603.png)

去马赛克：

​		对相邻同色的像素数值进内联插。举例来说，当芯片曝光得到一张影像后，每个像素就可以读取出来。绿色过滤器的像素精确测量了绿色成分，而该像素红色和蓝色的成分则是从邻区获取。

​		在颜色和亮度突变处却会产生噪声，比如渗色（Color bleeding），在锐利的边角处特别明显。

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706154518116.png" alt="image-20210706154518116" style="zoom: 50%;" />

### 1.3 HSV色彩空间

​		HSV是根据颜色的直观特性由A.R.Smith在1978年创造的一种颜色空间，也称六角锥体模型。这个模型中颜色的参数分别是：色调(H)、饱和度(S)、明度(V)。
​	 色调H：用角度度量，取值范围为0°~360°，从红色开始按逆时针方向计算，红色为0°，绿色为120°，蓝色为240°。它们的补色是：黄色为60°，青色为180°，紫色为360°。
​	 饱和度S：饱和度S表示颜色接近光谱色的程度。一种颜色，可以看成是某种光谱色与白色混合的结果。其中光谱色所占的比例越大，颜色接近光谱色的程度就越高，颜色的饱和程度也就越高，饱和度高，颜色则深而艳。光谱色的白光成分为0，饱和度达到最高。通常取值范围为0%~100%，值越大，颜色越饱和。
 	明度V：明度表示颜色明亮的程度，对于光源色，明度值与发光体的光亮度有关；对于物体色，此值和物体的透射比或反射比有关。通常取值范围为0%（黑）到100%（白）。

<img src="https://tse1-mm.cn.bing.net/th/id/R-C.92d3b19b81ecfc27b54819a0e9603f86?rik=dlE%2bKjzxGcvHUQ&riu=http%3a%2f%2fimg.blog.csdn.net%2f20150315145428311&ehk=7NA4BCIHG03cvecHPKdzNIhInn63TvtC9WueV7McdfY%3d&risl=&pid=ImgRaw" alt="查看源图像" style="zoom: 25%;" />

## 第二节——变换

### 2.1 坐标系变换

​		图像处理、立体视觉等等方向常常涉及到四个坐标系：世界坐标系、相机坐标系、图像坐标系、像素坐标系。例如下图：  

<img src="https://img-blog.csdn.net/20161210141741307?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" style="zoom: 67%;" />

​		构建世界坐标系只是为了更好的描述相机的位置在哪里，在双目视觉中一般将世界坐标系原点定在左相机或者右相机或者二者X轴方向的中点。 
​		接下来的重点，就是关于这几个坐标系的转换。也就是说，一个现实中的物体是如何在图像中成像的。

#### 1.1世界坐标系与相机坐标系

<img src="https://img-blog.csdn.net/20161210141917167?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" style="zoom: 67%;" />

​		于是，从世界坐标系到相机坐标系，涉及到旋转和平移（其实所有的运动也可以用旋转矩阵和平移向量来描述）。绕着不同的坐标轴旋转不同的角度，得到相应的旋转矩阵，如下图所示：   

<img src="https://img-blog.csdn.net/20161210142306747?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" style="zoom:67%;" />

​		那么从世界坐标系到相机坐标系的转换关系如下所示：   

<img src="https://img-blog.csdn.net/20161210142437060?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" style="zoom:67%;" />

#### 1.2相机坐标系与图像坐标系

​		从相机坐标系到图像坐标系，属于透视投影关系，从3D转换到2D。  

<img src="https://img-blog.csdn.net/20161210142740999?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" style="zoom:67%;" />

​		此时投影点p的单位还是mm，并不是pixel，需要进一步转换到像素坐标系。

#### 1.3图像坐标系与像素坐标系

​		像素坐标系和图像坐标系都在成像平面上，只是各自的原点和度量单位不一样。图像坐标系的原点为相机光轴与成像平面的交点，通常情况下是成像平面的中点或者叫principal point。图像坐标系的单位是mm，属于物理单位，而像素坐标系的单位是pixel，我们平常描述一个像素点都是几行几列。所以这二者之间的转换如下：其中dx和dy表示每一列和每一行分别代表多少mm，即`1(pixel)=dx(mm)`

<img src="https://img-blog.csdn.net/20161210143514044?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" style="zoom:67%;" />

​		那么通过上面四个坐标系的转换就可以得到一个点从世界坐标系如何转换到像素坐标系的。

![这里写图片描述](https://img-blog.csdn.net/20161210144703071?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbnRyYXZlbGxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 2.2 图像的几何变换

​		**刚体变换(rigid transformation):** 旋转和平移变换（rotation,translation）, 3个自由度，点与点之间的距离不变

$$
x = {\begin{pmatrix}
R&t\\
0^T&1\\
\end{pmatrix}}*x'
$$

 R为2*2旋转矩阵，t为2维列向量，0^T为0的二维行向量

​		**相似变换(similarity transformation):** 增加了缩放尺度, 四个自由度，点与点之间的距离比不变，如下，其中s为缩放尺度。

$$
x = {\begin{pmatrix}
sR&t\\
0^T&1\\
\end{pmatrix}}*x'
$$

​		**仿射变换(affine transformation):** 仿射变换和相似变换近似，不同之处在于相似变换具有单一旋转因子和单一缩放因子，仿射变换具有两个旋转因子和两个缩放因子，因此具有6个自由度. 不具有保角性和保持距离比的性质，但是原图平行线变换后仍然是平行线.

$$
x = {\begin{pmatrix}
A&t\\
0^T&1\\
\end{pmatrix}}*x'
$$

A为2*2的非奇异矩阵,可被分解为如下: $A=R(θ)R(−ϕ)DR(ϕ)$

其中$R(θ)R(ϕ)$ 为旋转矩阵，D为对角阵$D={\begin{pmatrix}λ1&0\\0&λ2\\\end{pmatrix}}$

$λ1$和$λ2$可以看做两个方向的缩放比.

​		**投影变换(projective transformation):** 也叫作单应性变换。投影变换是齐次坐标下非奇异的线性变换。然而在非齐次坐标系下却是非线性的，这说明齐次坐标的发明是很有价值的。投影变换比仿射变换多2个自由度，具有8个自由度。上面提到的仿射变换具有的“不变”性质，在投影变换中已不复存在了。尽管如此，它还是有一项不变性，那就是在原图中保持共线的3个点，变换后仍旧共线。**单应性变换其实就是一个平面到另一个平面的变换关系。**投影变换表示如下：

$$
x = {\begin{pmatrix}
A&t\\
V^T&1\\
\end{pmatrix}}*x'
$$

其中$V=(v1,v2)^T$

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706195746534.png" alt="image-20210706195746534" style="zoom:67%;" />

### 2.3 补充

​		相机成像时会产生桶型失真和枕型失真，产生原因和解决方法如下：

​		镜头成像原理图，图中左边的直线是目标，右边的直线是目标所成的像，中间是镜头。从图中可以看出，目标中心点O点成像于像的中心O'点，目标上不同的两点A点和B点成像于A'和B'点，且有

![img](https://bkimg.cdn.bcebos.com/formula/afe57da80cb4c99b692f89888b8588ae.svg)

​		即产生了桶形失真。根据牛顿成像定理，目标高度r与对应像高r'之间的关系为

![img](https://bkimg.cdn.bcebos.com/formula/d34645bf2e08e0fa403bce47e59fc347.svg)

​		在物距u一定的情况下, 焦距f愈大，像高r'愈大。镜头不能再等效为理想透镜，而是一个焦距随着目标离光轴距离而变化的成像系统。这样，随着目标离光轴距离的增加，焦距f随着减小，所成图像就产生了桶形失真。 [1] 

<img src="https://bkimg.cdn.bcebos.com/pic/b812c8fcc3cec3fdb1fce1fbdd88d43f86942779?x-bce-process=image/resize,m_lfit,w_1280,limit_1/format,f_auto" alt="图1　镜头成像原理图" style="zoom: 50%;" />

​		补救方法：采用多项式地址修正法作图像的映射变换，就是寻求畸变图像上一点A到理论图像上一点A'的映射关系，在实际中，我们采用对相机进行标定的方式解决，原理暂时不表

## 第三节——图像处理

### 3.1 图像滤波

​		由于图片在计算机中的形式一般是一个RGB矩阵，故而一张图片我们也可以将之抽象为一个函数模型，如图：

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706201857608.png" alt="image-20210706201857608" style="zoom:67%;" />

#### 3.1.1 像素处理

​		一般有亮度调整，对比度调整，颜色转变，直方图均衡化等……基本上是十多年前手机内集成的功能辣。

​		特别提一嘴直方图均衡化，**直方图均衡化**是图像处理进行调整的方法。这种方法通常用来增加许多图像的全局**对比度**，尤其是当图像的有用数据的对比度相当接近的时候。通过这种方法，**亮度**可以更好地在直方图上分布。这样就可以用于增强局部的对比度而不影响整体的对比度，直方图均衡化通过有效地扩展常用的亮度来实现这种功能。

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706202657933.png" alt="image-20210706202657933" style="zoom: 67%;" />

#### 3.1.2 图像（滤波）卷积

在空间域上，滤波功能可分为平滑图像，锐化图像，特征提取等，在频域上，滤波则是能起到噪点移除，重采样，图像压缩的效果。

* 线性滤波

  线性滤波类似于卷积，若有一个$1/9 * {\begin{bmatrix}1&1&1\\1&1&1\\1&1&1\\\end{bmatrix}}$的卷积核，此时滤波起到的效果是平滑整张图片，较为简单，这里不提，这里推导一下拉普拉斯核是如何提取图像边沿的。

  1. 拉普拉斯算子

&emsp;&emsp;对于一维离散函数$f(x)$，其一阶微分被定义为：$$\frac{\partial f}{\partial x} = f(x+1)-f(x)$$ 

&emsp;&emsp;那么，在此基础上，二阶微分可以被定义为：$$\frac{\partial^2 f}{\partial x^2} = f(x+1)+f(x-1)-2f(x)$$

&emsp;&emsp;采用逐差法，我们来看看边缘的灰度分布图以及将一二阶微分作用于边缘上时的效果：

![下载](C:\Users\DJW74\Desktop\下载.png)

​		我们可以看到，在边缘（也就是台阶处），二阶微分值非常大，其他地方值比较小或者接近0。于是我们就可以得到一个结论，微分算子的响应程度与图像在用算子操作的这一点的突变程度成正比，这样，图像微分增强边缘和其他突变（如噪声），而削弱灰度变化缓慢的区域。也就是说，微分算子（尤其是二阶微分），对边缘图像非常敏感。因此，我们可以在二维层面上对图像进行一阶和二阶微分，达到边缘提取的效果。

​		很多时候，我们最关注的是一种各向同性的滤波器，这种滤波器的响应与滤波器作用的图像的突变方向无关。而最简单的各向同性微分算子是拉普拉斯算子。一个二维的拉普拉斯算子定义为：$$\nabla^2 f= \frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}$$&emsp;&emsp;那么根据一维函数的一阶微分定义，我们可以得到：

$$
\because \frac{\partial^2 f}{\partial x^2} = f(x+1)+f(x-1)-2f(x)
$$
$$
\frac{\partial^2 f}{\partial y^2} = f(y+1)+f(y-1)-2f(y)
$$
$$
\therefore \nabla^2 f= \frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2} = f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)
$$

​		如果我们以$(x,y)$为中心点，来重新表达这个算子，那么会得到：$$\begin{bmatrix}0&1&0\\1&-4&1\\0&1&0\\\end{bmatrix}$$

​		仔细观察这个算子，我们会发现它的表现形式与卷积核一模一样！也就是说，将该算子与原图像进行卷积运算，我们就会得到一个经过微分的图像，再将原图像与经过微分的图像叠加在一起，我们就可以得到经过边缘提取后的图像，此时，卷积核就变为：$$
$$
\begin{bmatrix}0&0&0\\0&1&0\\0&0&0\\\end{bmatrix}-\begin{bmatrix}0&1&0\\1&  -  4&1\\0&1&0\\\end{bmatrix}  =  \begin{bmatrix}0&-1&0\\-1&5&-1\\0&-1&0\\\end{bmatrix}
$$
​	**这就是拉普拉斯算子提取边沿的原理**，实际运用中，可对卷积核进行一些小改动，从而取得图片锐化的效果。

2. 高斯算子

   若说拉普拉斯算子能够提取图像边沿，则高斯算子则可以对图像进行平滑化，其原理如下图所示：

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706215427873.png" alt="image-20210706215427873" style="zoom: 50%;" />

​		它同时考虑了中心和周围的权重，从而可以达到较好的平滑图像的效果。

**高斯滤波算子可分离性**

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706215654566.png" alt="image-20210706215654566" style="zoom:50%;" />

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706215727372.png" alt="image-20210706215727372" style="zoom:50%;" />

​		3. 其余边沿算子

​		横向边沿算子和纵向边缘算子的推导类似于拉普拉斯算子，但两者都只考虑了一个维度的边沿：

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706220013328.png" alt="image-20210706220013328" style="zoom:50%;" />

​	横向，纵向算子与拉普拉斯算子的比较：

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706220053659.png" alt="image-20210706220053659" style="zoom:50%;" />

​		此外，还有索贝尔算子，也是通过离散求导的思想推出的边沿算子，这里按下不表

* 非线性滤波

  除了线性滤波以外，非线性滤波也有一席之地，这里介绍一下在去除图像椒盐噪点上颇强的**中值**滤波

  <img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706220501552.png" alt="image-20210706220501552" style="zoom:67%;" />

* 图像的膨胀腐蚀

  ​	腐蚀膨胀是形态学图像处理的基础，腐蚀在二值图像的基础上做“收缩”或“细化”操作，膨胀在二值图像的基础上做“加长”或“变粗”的操作。

  * 腐蚀算法

    ​		腐蚀是一种消除边界点，使边界向内部收缩的过程。可以用来消除小且无意义的物体。用3X3的结构元素，扫描图像的每一个像素，用结构元素与其覆盖的二值图像做“与”操作，如果都为1，结果图像的该像素为1。否则为0。结果会使二值图像小一圈。

    有一个形象的比喻来可以说明该运算，用0表示蛀虫，1表示大米。蛀虫腐蚀大米的过程便是

![img](https://img2020.cnblogs.com/blog/2014896/202005/2014896-20200510161822640-547214320.png)

* 图像的膨胀腐蚀

  * 膨胀算法

    ​		膨胀是将与物体接触的所有背景点合并到该物体中，使边界向外部扩张的过程。可以用来填补物体中的空洞。用3X3的结构元素，扫描图像的每一个像素，用结构元素与其覆盖的二值图像做“与”操作，如果都为0，结果图像的该像素为0,。否则为1。结果使二值图像扩大一圈。

    ​		**先腐蚀后膨胀的过程称为开运算。用来消除小物体、在纤细点处分离物体、平滑较大物体的边界的同时并不明显的改变其面积。先膨胀后腐蚀的过程称为比运算，用来填充物体内细小空间、连接邻近物体、平滑其边界的同时并不明显改变其面积。**

    <img src="http://ee.mweda.com/imgqa/ele/FPGA/FPGA-3721rd.com-30681ooj4bmsyihp.png" alt="img" style="zoom:80%;" />

* 图像的频域滤波

众所周知，傅里叶变换可以对函数的时间（空间）域和其频率域进行互相转换，我们急用傅里叶变换，可以将图像从空间域转变为频率域，再对图像进行滤波，之后还原为空间域的图像，就能产生一些效果

* 低通滤波器：使低频通过而使高频衰减的滤波器 
  1. 被低通滤波的图像比原始图像少尖锐的细节部分而突出平滑过渡部分 
  2. 对比空间域滤波的平滑处理，如均值滤波器

* 高通滤波器：使高频通过而使低频衰减的滤波器 
  1. 被高通滤波的图像比原始图像少灰度级的平滑过渡而突出边缘等细节部分 
  2. 对比空间域的梯度算子、拉普拉斯算子

### 3.2 图像金字塔与图像融合

* 图像金字塔是多尺度表达的一种，是一种以多分辨率来解释图像的有效但概念简单的结构。。一幅图像的金字塔是一系列以金字塔形状排列的分辨率逐渐降低并且来源于同一张原始图像的集合。通过梯次向下采样获得，直到某个终止条件才停止采样。
* 图像金字塔说白了就是披着金字塔外衣的图像缩放。一般有高斯图像金字塔、拉普拉斯图像金字塔。

#### 3.2.1 高斯金字塔

​		高斯金字塔：用于下采样。高斯金字塔是最基本的图像塔。
​		原理：首先将原图像作为最底层图像G0（高斯金字塔的第0层），利用高斯核（5*5）对其进行卷积（高斯平滑），然后对卷积后的图像进行下采样（去除偶数行和列）得到上一层图像G1，将此图像作为输入，重复卷积和下采样操作得到更上一层图像，反复迭代多次，形成一个金字塔形的图像数据结构，即高斯金字塔。（若直接进行下采样，则会使得图像一些低频信息被滤除，导致图像边沿不平滑，损失一些信息，无法较好地还原）。

​		由于上采样和下采样是非线性处理，是不可逆的有损处理，因此下采样后的图像想要还原回原来的尺寸的话会丢失很多信息，使图片变模糊。

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706234455146.png" alt="image-20210706234455146" style="zoom:50%;" />

#### 3.2.2 拉普拉斯金字塔

​		拉普拉斯金字塔：用于重建图像，也就是预测残差，对图像进行最大程度的还原。
​		一幅小图像重建为一幅大图原理：用高斯金字塔的每一层图像减去其上一层图像上采样并高斯卷积之后的预测图像，得到一系列的差值图像即为 LP 分解图像。

​		拉普拉斯金字塔实际上是通过计算图片先下采样再上采样后的结果和原图片的残差来保存缺失信息的，公式为：
$$
L(i)=G(i)−PyrUp(G(i+1))
$$

​		也就是说，拉普拉斯金字塔实际上是由上面的残差图片组成的金字塔，**它为还原图片做准备**。
​		求得每个图像的拉普拉斯金字塔后需要对相应层次的图像进行融合，最终还原图像。
​		**拉普拉斯金字塔可以精确还原图片信息。**

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706235458316.png" alt="image-20210706235458316" style="zoom:33%;" />

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706235438699.png" alt="image-20210706235438699" style="zoom:33%;" />

拉普拉斯金子塔如下所示：

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706235550145.png" alt="image-20210706235550145" style="zoom:67%;" />

利用拉普拉斯金字塔还原图像如图所示意：

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210706235816843.png" alt="image-20210706235816843" style="zoom:67%;" />

#### 3.2.3 图像融合

​		图像融合的目的就是使两幅图像的重叠区域过渡自然且平滑。
​		常见融合方法：

1. 加权平均法。这个很好理解，即简单的使用加权的方式从左边过渡到右边。这种方法效果一般，但算法实现极其简单，速度快。
2. 羽化算法 。这种方法过渡会比加权平均法自然，但会造成不好的模糊效果。
3. 拉普拉斯金字塔融合。有的地方也称为多分辨率融合算法。这种方法是将图像建立一个拉普拉斯金字塔，其中金字塔的每一层都包含了图像不同的频段。分开不同频段进行融合效果出奇的好。

**图像拉普拉斯金字塔分解的目的是将源图像分别分解到不同的空间频带上，融合过程是在各空间频率层上分别进行的，这样就可以针对不同分解层的不同频带上的特征与细节，采用不同的融合算子以达到突出特定频带上特征与细节的目的。即有可能将来自不同图像的特征与细节融合在一起。**

比如说，一个人的头发为黑色，黑色就是人头图像上的高频特征，而头发具体的纹理则是图像上的低频特征，利用金字塔，可以在高频（黑色）和低频（纹理）以及中间频带上融合，将两者结合，达到更好的融合效果。

## 第四节——特征检测

### 4.1边缘特征

边缘是图像亮度、颜色或纹理快速变化的地方，在图像上通常表示为：对象的边界，阴影的边界以及折痕，边缘特征提取的方法较多，较普遍的方法还是逃不开上面讲的先用二元函数方程表达边缘，再将之化为卷积核的方法（详见拉普拉斯算子推导）

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210707010359951.png" alt="image-20210707010359951" style="zoom:67%;" />

这里介绍三种新方法：

#### 4.1.1 canny边缘检测

Canny边缘检测主要分四步进行：

1. 去噪声；
2. 计算梯度与方向角；
3. 非最大值抑制；
4. 滞后阈值化；

其中前两步很简单，先用一个高斯滤波器对图像进行滤波，然后用Sobel水平和竖直检测子与图像卷积，来计算梯度和方向角。

* 非极大值抑制

图像梯度幅值矩阵中的元素值越大，说明图像中该点的梯度值越大，但这不不能说明该点就是边缘（这仅仅是属于图像增强的过程）。在Canny算法中，非极大值抑制是进行边缘检测的重要步骤，通俗意义上是指寻找像素点局部最大值，将非极大值点所对应的灰度值置为0，这样可以剔除掉一大部分非边缘的点。

![image](https://images0.cnblogs.com/blog/378920/201410/012004319096476.png)

根据上图可知，要进行非极大值抑制，就首先要确定像素点C的灰度值在其8值邻域内是否为最大。图中蓝色的线条方向为C点的梯度方向，这样就可以确定其局部的最大值肯定分布在这条线上，也即出了C点外，梯度方向的交点dTmp1和dTmp2这两个点的值也可能会是局部最大值。因此，判断C点灰度与这两个点灰度大小即可判断C点是否为其邻域内的局部最大灰度点。如果经过判断，C点灰度值小于这两个点中的任一个，那就说明C点不是局部极大值，那么则可以排除C点为边缘。这就是非极大值抑制的工作原理。

在理解的过程中需要注意以下两点：

1. 中非最大抑制是回答这样一个问题：“当前的梯度值在梯度方向上是一个局部最大值吗？” 所以,要把当前位置的梯度值与梯度方向上两侧的梯度值进行比较；
2. 梯度方向垂直于边缘方向。

* 滞后阈值化

由于噪声的影响，经常会在本应该连续的边缘出现断裂的问题。滞后阈值化设定两个阈值：一个为高阈值$T_h$，一个为低阈值$T_l$。如果任何像素边缘算子的影响超过高阈值，将这些像素标记为边缘；响应超过低阈值（高低阈值之间）的像素，如果与已经标记为边缘的像素4-邻接或8-邻接，则将这些像素也标记为边缘。所以不整个过程描述如下：

1. 如果该像素的梯度值小于$T_l$，则该像素为非边缘像素；
2. 如果该像素的梯度值大于$T_h$，则该像素为边缘像素；
3. 如果该像素的梯度值介于$T_l$与$T_h$之间，需要进一步检测该像素的$3×3$邻域内的8个点，如果这8个点内有一个或以上的点梯度超过了$T_h$，则该像素为边缘像素，否则不是边缘像素。

#### 4.1.2 LoG & DoG

LoG即为拉普拉斯算子与高斯算子相乘所得，而DoG则为两个不同参数的高斯算子相减所得，如下所示：

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210707011941565.png" alt="image-20210707011941565" style="zoom: 50%;" />

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210707012024470.png" alt="image-20210707012024470" style="zoom:50%;" />

#### 4.1.3 霍夫变换

霍夫变换主要用于，对付图片数据，能够提供一种方式找到直线/圆，广义上的霍夫变换可以找到你想要的任何你可以描述的特征。

　　**基本原理**

一条直线可由两个点A=(X1,Y1)和B=(X2,Y2)确定(笛卡尔坐标)

![img](https://pic2.zhimg.com/80/v2-0a8ff5fa59b5082f1e77a38b1c09982d_720w.jpg)

另一方面，y = kx + q，也可以写成关于（k,q）的函数表达式（霍夫空间）：

![img](https://pic4.zhimg.com/80/v2-d103ec2968ed0a6bc61cb7b5d05e1cb7_720w.png)

对应的变换可以通过图形直观表示：

![img](https://pic4.zhimg.com/80/v2-92dff3e1b0f3aaa90b5e5bc82592d627_720w.jpg)

变换后的空间成为**霍夫空间**。即：**笛卡尔坐标系中一条直线，对应霍夫空间的一个点**。

反过来同样成立（**霍夫空间的一条直线，对应笛卡尔坐标系的一个点**）：

![img](https://pic3.zhimg.com/80/v2-adbd9d4761fcb9d9fd49a3dbb1bce696_720w.jpg)

再来看看A、B两个点，对应霍夫空间的情形：

![img](https://pic1.zhimg.com/80/v2-262e660a281b36ce0a60289479e9b738_720w.jpg)

一步步来，再看一下三个点共线的情况：

![img](https://pic1.zhimg.com/80/v2-7966de4cc1966a517aefa816dc6a80b8_720w.jpg)

可以看出如果笛卡尔坐标系的点共线，这些点在霍夫空间对应的直线交于一点：这也是必然，共线只有一种取值可能。

如果不止一条直线呢？再看看多个点的情况（有两条直线）：

![img](https://pic1.zhimg.com/80/v2-bab3a0f9416165a792d6d12b86773ed4_720w.jpg)

其实（3，2）与（4，1）也可以组成直线，只不过它有两个点确定，而图中A、B两点是由三条直线汇成，这也是**霍夫变换的后处理的基本方式**：**选择由尽可能多直线汇成的点**。

看看，霍夫空间：选择由三条交汇直线确定的点（中间图），对应的笛卡尔坐标系的直线（右图）。

![img](https://pic1.zhimg.com/80/v2-ffef6c491efa60c5515d3315e086c528_720w.jpg)

到这里问题似乎解决了，已经完成了霍夫变换的求解，但是如果像下图这种情况呢？

![img](https://pic2.zhimg.com/80/v2-89b6d00ee421cbfaa95ce9ce594db245_720w.jpg)

k=∞是不方便表示的，而且q怎么取值呢，这样不是办法。因此考虑**将笛卡尔坐标系换为：极坐标表示**。

![img](https://pic4.zhimg.com/80/v2-d49fbe783be10547841decc66e4fdc27_720w.jpg)

在极坐标系下，其实是一样的：极坐标的点→霍夫空间的直线，只不过霍夫空间不再是$[k,q]$的参数，而是**$[\rho, \theta]$**的参数，给出对比图：

![img](https://pic2.zhimg.com/80/v2-434403a32e6626fec1484ddb6b03df21_720w.jpg)

在极坐标系中表示直线的方程为ρ=xCosθ+ySinθ（ρ为原点到直线的距离）,如图所示：

**我们遍历θ的值，从0-180°，并且同时代入（x,y）的值，求得对应的ρ。**

**找到0-180°中，哪个度数下的ρ值相同的数量最多。这反向说明了，在一个ρ和θ组成的函数中，符合的点数最多。**

### 4.2 角点特征

我们都玩过拼图游戏，将大量混乱的小片段进行正确地排列，形成一个完整的图像。

玩这个游戏的关键点是什么？就是寻找那些区分度比较高的特征片段，它们更容易用于定位。

那这些有效特征到底是什么？

回答这个问题之前，先看一下下面这张图片：

<img src="https://pic2.zhimg.com/80/v2-f9eeb6af0d32d77a2f4ad0c9887359d5_720w.jpg" alt="img" style="zoom: 67%;" />



在图像的顶部，给出了它的六个图像片段。我们试试如何在原始图像中找到这些图像片段的确切位置。

如果你亲自试过的话，可以发现：

- A和B处于平坦区域，没有什么确切的特征，它们所在的位置有很多种可能；
- C和D要相对简单一些，它们是建筑物的边缘，我们可以找到一个大致的位置，但是要定位到精确的位置仍然很难。所以边缘是更好的特征，但还不够好。
- E和F是建筑的一些角落，可以很容易地发现它们的位置，因为对于建筑物角落这个图像片段，我们不管朝哪个方向移动，这个片段看起来都会不一样。

简化一下，就可以得到这张图：

![img](https://pic4.zhimg.com/80/v2-bb0946575a969ed75783b452e9c0ed1f_720w.jpg)



- 蓝色矩形表示一个平坦区域，在各方向移动，窗口内像素值没有变化；
- 黑色矩形表示一个边缘特征（Edges），如果沿着垂直方向移动(梯度方向)，像素值会发生改变；如果沿着边缘移动(平行于边缘) ，像素值不会发生变化；
- 对于红色矩形框来说，它是一个角（Corners），不管你把它朝哪个方向移动，像素值都会发生很大变化。

三种情况分别如下图所示：

![img](https://pic2.zhimg.com/80/v2-4cb0479e245d307e6b2ffe07f38067fd_720w.jpg)



图像特征提供了图像丰富的信息。角点特征是图像中较好的特征，比边缘特征更好地用于定位。

对于拼图游戏，我们已经知道了什么是好的特征，但是下一个问题出现了，如何找到这些角点特征。

找到这些图像特征的过程被称为**特征提取**，特征提取决定了最终目标识别效果的好坏。

在图像的所有区域中，那些在所有方向上做微小移动，像素值变化都很大的区域，就是角点特征所在区域。

检测角点的应用主要有：图像对齐、图像拼接(拍摄全景图)、目标识别、3D重建、运动跟踪等等。

#### 4.2.1 Harris 角点检测器

在图像处理中，检测角点特征的算法有很多，这里先介绍最常用的，也是最基础的 Harris 角点检测器(Harris Corner Detection)。

角点是两条边缘的交点，它表示两条边方向改变的地方，所以**角点在任意一个方向上做微小移动，都会引起该区域的梯度图的方向和幅值发生很大变化。**

也就是一阶导数(即灰度图的梯度)中的局部最大所对应的像素点就是角点。

于是我们可以利用这一点来检测角点。

使一个固定尺寸的窗口在图像上某个位置以任意方向做微小滑动，如果窗口内的灰度值（在梯度图上）都有较大的变化，那么这个窗口所在区域就存在角点。

这样就可以将 Harris 角点检测算法分为以下三步：

1. 当窗口（小的图像片段）同时向 x 和 y 两个方向移动时，计算窗口内部的像素值变化量 ![[公式]](https://www.zhihu.com/equation?tex=E%28u%2C+v%29) ；
2. 对于每个窗口，都计算其对应的一个角点响应函数 R；
3. 然后对该函数进行阈值处理，如果 R > threshold，表示该窗口对应一个角点特征。

接下来对每一步进行详细介绍。

##### Harris检测第一步

如何确定哪些窗口会引起较大的灰度值变化？

让一个窗口的中心位于灰度图像的一个位置 ![[公式]](https://www.zhihu.com/equation?tex=%28x%2C+y%29) ，这个位置的像素灰度值为 ![[公式]](https://www.zhihu.com/equation?tex=I%28x%2C+y%29) ，如果这个窗口分别向 x 和 y 方向移动一个小的位移u和v，到一个新的位置 ![[公式]](https://www.zhihu.com/equation?tex=%28x%2Bu%2C+y%2Bv%29) ，这个位置的像素灰度值就是 ![[公式]](https://www.zhihu.com/equation?tex=I%28x%2Bu%2C+y%2Bv%29) 。

![[公式]](https://www.zhihu.com/equation?tex=%5BI%28x%2Bu%2C+y%2Bv%29+-+I%28x%2C+y%29%5D) 就是窗口移动引起的灰度值的变化值。

设 ![[公式]](https://www.zhihu.com/equation?tex=w%28x%2Cy%29) 为位置 ![[公式]](https://www.zhihu.com/equation?tex=%28x%2Cy%29) 处的窗口函数，表示窗口内各像素的权重，最简单的就是把窗口内所有像素的权重都设为1。

有时也会把 ![[公式]](https://www.zhihu.com/equation?tex=w%28x%2Cy%29) 设定为以窗口中心为原点的高斯分布（二元正态分布）。如果窗口中心点像素是角点，那么窗口移动前后，中心点的灰度值变化非常强烈，所以该点权重系数应该设大一点，表示该点对灰度变化的贡献较大；而离窗口中心（角点）较远的点，这些点的灰度变化比较小，于是将权重系数设小一点，表示该点对灰度变化的贡献较小。

则窗口在各个方向上移动 ![[公式]](https://www.zhihu.com/equation?tex=%28u%2Cv%29) 所造成的像素灰度值的变化量公式如下：

![[公式]](https://www.zhihu.com/equation?tex=E%28u%2Cv%29+%3D+%5Csum%5Climits_%7B%28x%2Cy%29%7D+w%28x%2Cy%29+%5Ctimes+%5BI%28x%2Bu%2C+y%2Bv%29+-+I%28x%2Cy%29%5D%5E2+%5C%5C)

对于一个角点来说， ![[公式]](https://www.zhihu.com/equation?tex=E%28u%2Cv%29) 会非常大。因此，我们可以最大化上面这个函数来得到图像中的角点。

用上面的函数计算 ![[公式]](https://www.zhihu.com/equation?tex=E%28u%2Cv%29) 会非常慢。因此，我们使用泰勒展开式（只有一阶）来得到这个公式的近似形式。

对于二维的泰勒展开式公式为：

![[公式]](https://www.zhihu.com/equation?tex=T%28x%2Cy%29+%5Capprox+f%28u%2Cv%29+%2B+%28x-u%29f_x%28u%2Cv%29+%2B+%28y-v%29f_y%28u%2Cv%29+%2B+...+%5C%5C)

将 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+I%28u%2Bx%2Cv%2By%29) 套用上面的公式，可以得到：

![[公式]](https://www.zhihu.com/equation?tex=I%28x%2Bu%2Cy%2Bv%29+%5Capprox+I%28x%2Cy%29+%2B+uI_x+%2B+vI_y+%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=I_x) 和 ![[公式]](https://www.zhihu.com/equation?tex=I_y) 是 ![[公式]](https://www.zhihu.com/equation?tex=I) 的偏微分，在图像中就是在 x 和 y 方向的**梯度图**（可以通过`cv2.Sobel()`来得到）：

![[公式]](https://www.zhihu.com/equation?tex=I_x+%3D+%5Cfrac+%7B%5Cpartial+I%28x%2Cy%29%7D%7B%5Cpartial+x%7D%2C%5C+%5C+I_y+%3D+%5Cfrac+%7B%5Cpartial+I%28x%2Cy%29%7D%7B%5Cpartial+y%7D+%5C%5C)

接下来继续推导：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++E%28u%2Cv%29+%26%3D+%5Csum%5Climits_%7B%28x%2Cy%29%7D+w%28x%2Cy%29+%5Ctimes+%5BI%28x%2Cy%29+%2B+uI_x+%2B+vI_y+-+I%28x%2Cy%29%5D%5E2+%5C%5C+%26%3D+%5Csum%5Climits_%7B%28x%2Cy%29%7D+w%28x%2Cy%29+%5Ctimes+%28uI_x+%2B+vI_y%29%5E2+%5C%5C++%26%3D+%5Csum%5Climits_%7B%28x%2Cy%29%7D+w%28x%2Cy%29+%5Ctimes+%28u%5E2I_x%5E2+%2B+v%5E2I_y%5E2+%2B+2uvI_xI_y%29+%5Cend%7Baligned%7D+%5C%5C)

把 u 和 v 拿出来，得到最终的形式：

![[公式]](https://www.zhihu.com/equation?tex=E%28u%2Cv%29+%5Capprox+%5Cbegin%7Bbmatrix%7D+u%2C+v+%5Cend%7Bbmatrix%7D+M+%5Cbegin%7Bbmatrix%7D+u+%5C%5C+v+%5Cend%7Bbmatrix%7D+%5C%5C)

其中矩阵M为：

![[公式]](https://www.zhihu.com/equation?tex=M+%3D+%5Csum%5Climits_%7B%28x%2Cy%29%7D+w%28x%2Cy%29++%5Cbegin%7Bbmatrix%7D+I_x%5E2+%26+I_xI_y+%5C%5C+I_xI_y+%26+I_y%5E2+%5Cend%7Bbmatrix%7D++%5Crightarrow+R%5E%7B-1%7D+%5Cbegin%7Bbmatrix%7D+%5Clambda_1+%26+0+%5C%5C+0+%26+%5Clambda_2+%5Cend%7Bbmatrix%7D+R++%5C%5C)

最后是把实对称矩阵对角化处理后的结果，可以把R看成旋转因子，其不影响两个正交方向的变化分量。

经对角化处理后，将两个正交方向的变化分量提取出来，就是 λ1 和 λ2（特征值）。

公式推导完了，现在回顾一下：

对于图像的每一个像素点 ![[公式]](https://www.zhihu.com/equation?tex=%28x%2Cy%29) ，对应一个以该像素为中心的窗口 ![[公式]](https://www.zhihu.com/equation?tex=w%28x%2Cy%29) ，然后该像素平移 ![[公式]](https://www.zhihu.com/equation?tex=%28u%2Cy%29) 得到新的像素点 ![[公式]](https://www.zhihu.com/equation?tex=%28x%2Bu%2C+y%2Bv%29) ，而 ![[公式]](https://www.zhihu.com/equation?tex=E%28u%2Cv%29) 就是**窗口中所有像素的加权和乘以不同位置像素的灰度差值**。

##### Harris检测第二步

现在我们已经得到 ![[公式]](https://www.zhihu.com/equation?tex=E%28u%2Cv%29) 的最终形式，别忘了我们的目的是要找到会引起较大的灰度值变化的那些窗口。

灰度值变化的大小则取决于矩阵M，那么如何找到这些窗口，我们可以使用矩阵的特征值来实现。

计算每个窗口对应的得分（角点响应函数R）：

![[公式]](https://www.zhihu.com/equation?tex=R+%3D+%5Ctext+%7Bdet%7D%28M%29+-+k+%28%5Ctext+%7Btrace%28M%29%7D%29%5E2+%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext+%7Bdet%7D%28M%29+%3D+%5Clambda_1+%5Clambda_2) 是矩阵的行列式， ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext+%7Btrace%28M%29%7D+%3D+%5Clambda_1+%2B+%5Clambda_2) 是矩阵的迹。

λ1 和 λ2 是矩阵M的特征值， ![[公式]](https://www.zhihu.com/equation?tex=k) 是一个经验常数，在范围 (0.04, 0.06) 之间。

##### Harris检测第三步

根据 R 的值，将这个窗口所在的区域划分为平面、边缘或角点。为了得到最优的角点，我们还可以使用**非极大值抑制**。

注意：Harris 检测器具有旋转不变性，但不具有尺度不变性，也就是说尺度变化可能会导致角点变为边缘，如下图所示：

![img](https://pic1.zhimg.com/80/v2-eeb16e97576beaa149b2db469cd82b64_720w.jpg)

（插播一句，想要尺度不变特性的话，可以关注SIFT特征）

因为特征值 λ1 和 λ2 决定了 R 的值，所以我们可以用特征值来决定一个窗口是平面、边缘还是角点：

- **平面:** 该窗口在平坦区域上滑动，窗口内的灰度值基本不会发生变化，所以 ![[公式]](https://www.zhihu.com/equation?tex=%7CR%7C) 值非常小，在水平和竖直方向的变化量均较小，即 ![[公式]](https://www.zhihu.com/equation?tex=I_x) 和 ![[公式]](https://www.zhihu.com/equation?tex=I_y) 都较小，那么 λ1 和 λ2 都较小；
- **边缘:**![[公式]](https://www.zhihu.com/equation?tex=R) 值为负数，仅在水平或竖直方向有较大的变化量，即 ![[公式]](https://www.zhihu.com/equation?tex=I_x) 和 ![[公式]](https://www.zhihu.com/equation?tex=I_y) 只有一个较大，也就是 λ1>>λ2 或 λ2>>λ1；
- **角点:**![[公式]](https://www.zhihu.com/equation?tex=R) 值很大，在水平、竖直两个方向上变化均较大的点，即 ![[公式]](https://www.zhihu.com/equation?tex=I_x) 和 ![[公式]](https://www.zhihu.com/equation?tex=I_y) 都较大，也就是 λ1 和 λ2 都很大

用图片表示如下：

![img](https://pic3.zhimg.com/80/v2-95cc2824eac2378cc04bf9282dbec912_720w.jpg)

Harris 角点检测的结果是带有这些分数 R 的灰度图像，设定一个阈值，分数大于这个阈值的像素就对应角点。

##### Harris检测的一些性质

* **阈值决定角点的数量**
* **光照不变性**：**Harris角点检测算子对亮度和对比度的变化不敏感（光照不变性）** 在进行Harris角点检测时，使用了微分算子对图像进行微分运算，而微分运算对图像密度的拉升或收缩和对亮度的抬高或下降不敏感。
* **旋转不变性：Harris角点检测算子具有旋转不变性** Harris角点检测算子使用的是角点附近的区域灰度二阶矩矩阵。而二阶矩矩阵可以表示成一个椭圆，椭圆的长短轴正是二阶矩矩阵特征值平方根的倒数。当特征椭圆转动时，特征值并不发生变化，所以判断角点响应值也不发生变化，由此说明Harris角点检测算子具有旋转不变性。
* **不具有尺度不变性：Harris角点检测算子不具有尺度不变性** 尺度的变化会将角点变为边缘，或者边缘变为角点，Harris的理论基础并不具有尺度不变性。

### 4.3 斑点特征

#### 4.3.1 从边到斑

我们直接检测斑点是比较困难的，所以，“聪明的”计算机学者想出了一种“巧妙的”方法来检测斑点——通过检测斑点两边的边缘来提取斑点。

对于阶跃信号，其二阶导数在对应跃变得位置会产生一个波，该波的过零点正对应着待检测的边缘位置。

![enter description here](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114126152-996774077.png)

而所谓的斑点区域就是两条边形成的中间区域，如下图中，对于蓝色的目标区域，红色的椭圆就是在检测边缘，黑色的椭圆就是在检测斑点区域。

![enter description here](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114126668-722699900.png)

这里，我们选用LoG对斑点的边沿进行检测，边缘检测到斑点检测的过程对二阶导数有什么影响呢？![enter description here](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114127512-1799179184.png)

这个示意图中上一行从左到右表示的是原始矩形脉冲信号，脉冲信号的周期越来越短，而下一行则表示对应周期在相同尺度下的LoG结果。可以发现总的LoG曲线其实是两条边界上产生的LoG函数的叠加，当两条边界足够小时，在该尺度脉冲下就被作为了blob，这时候LoG曲线的极值就对应着blob的中心。

所以边缘检测对应的是LoG的过零点，而斑点检测对应的是LoG的极值点。

这里我们可以发现判断是不是blob时，尺度非常重要，只有尺度大于一定值得时候blob才可以通过LoG的极值判定。那么尺度该如何选择呢？

#### 4.3.2 尺度的选择

先来看看同一个阶跃信号在不同尺度下的响应

![enter description here](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114128246-630188322.png)

可以发现随着尺度的不断增大，LoG曲线由双波谷逐渐融合成单波谷，但是响应的幅值越来越弱。这是因为，随着尺度的增大，LoG算子的最大幅度逐渐减小，导致响应也随着尺度的增大而减小。

这种情况我们没法知道选定的尺度是否合适，进而不知道这个尺度下找到的极值点是不是对应着blob的中心点，万一是边缘产生波的极值点怎么办！所以我们这时候应该对进行LoG算在进行尺度的归一化。

下图给出了阶跃信号的一阶Gaussian滤波曲线

![enter description here](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114128981-1568347117.png)

可以发现对于稳定信号，响应的最大幅值是和![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114129871-16663004.png)成正比的，那么为了消除尺度的影响，可以对一阶高斯滤波算子乘上个![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114130231-189871151.png),那么对于不同的尺度，其响应的强度就是相同的了。这里可以这么理解，在0点处积分结果为

![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114130809-1658521344.png)

是![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114131387-156774251.png)的函数表示与尺度有关，乘上一个![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114131902-433226037.png)之后就与尺度无关了，称为**尺度的归一化**。

而LoG算子是Gaussian 滤波的二阶导数，所以应该乘上个![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114132356-1648096182.png)做尺度归一化。

再来看看矩形脉冲在不同尺度经过尺度归一化后的响应

![enter description here](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114133168-1345647549.png)

可以发现当尺度等于脉冲宽度的一半的时候，不仅极值对应着blob的中心位置，而且这时候的响应强度比其他尺度响应强度都强。

那么我们在检测blob时就可以使用不同的尺度计算HoG响应，选择产生最强响应的尺度，在该尺度上对应的极值就是blob的中心位置了。

对于不同尺寸的blob，理想的尺度应该是多少呢？
理论表明，对于一个圆形blob，当二维LoG算子的零点值曲线和目标圆形边缘重合时取得最强响应。

下图是LoG剖面示意

![enter description here](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114133809-398060780.png)

所以由LoG算子的目标式![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114134434-457373451.png)可得最优尺寸为![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114134824-1130200409.png).

在使用LoG算子进行Blob检测时，首先在不同尺寸上对图像进行LoG，然后检测在尺度空间和图像空间都是极值的点，就是blob区域的中心点。

#### 4.3.3 LoG to DoG

为什么DoG可以近似LoG使用呢？
先来看看归一化之后的LoG算子(该过程将高斯函数带入化简可得)

![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114135684-936337656.png)

而高斯差分算子

![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114136371-335842091.png)

所以

![img](https://images2015.cnblogs.com/blog/1027162/201609/1027162-20160921114136840-539403997.png)

因为k-1是常量不影响函数极值点，所以检测归一化的LoG在尺度空间的极值，即近似于检测DoG空间的极值。

#### 4.3.4 DoG进行Blob检测流程

* 首先使用不同尺度的高斯算子对图像进行平滑
* 其次计算相邻尺度下平滑图像的差分图像(DoG空间)
* 最后在DoG空间寻找极值点

### 4.4 图像匹配中的RANSAC算法

#### 4.4.1前言

**单应矩阵求解方法主要有：**

1. 直接线性变换法
   1. 8点法
   2. 最小二乘法

2. 基于RANSAC的鲁棒方法

先简单介绍一下直接线性变换法：

<img src="https://pic4.zhimg.com/80/v2-043b75f09752b9c21aa7b46dccc29263_720w.jpg" alt="img" style="zoom:50%;" />

<img src="https://pic4.zhimg.com/80/v2-bb886b0e8c1c2ccbbc37a1a99542b58b_720w.jpg" alt="img" style="zoom:67%;" />

在n>=8时，由于噪点匹配对的存在，最小二乘法往往不会得到一个较优解，这时，就轮到我们的RANSAC算法闪亮登场了。

Some Q&A：

1、如何对匹配好的点做进一步的处理，更好保证匹配效果

答：1、汉明距离应小于最小距离两倍（二进制的描述子，不同位数的个数为汉明距离）  2、RANSAC消除误匹配

2、RANSAC与最小二乘区别：

最小二乘法尽量去适应包括局外点在内的所有点。相反，RANSAC能得出一个仅仅用局内点计算出模型，并且概率还足够高。但是，RANSAC并不能保证结果一定正确，为了保证算法有足够高的合理概率，必须小心的选择算法的参数（参数配置）。经实验验证，对于包含80%误差的数据集，RANSAC的效果远优于直接的最小二乘法。

#### 4.4.2 RANSAC算法简介

基本假设是样本中包含正确数据（inliers），也包含异常数据（outliers）。根据一组正确的数据，可以计算出符合这些数据的模型参数的方法。

**优点：**能鲁棒的估计模型参数。

**缺点：**1、计算参数的迭代次数没有上限，如果设置迭代次数的上限，得到的结果可能不是最优。

​			2、 RANSAC只有一定概率得到可信的模型，概率与迭代次数成正比。

RANSAC是“Random Sample Consensus（**随机一致性采样**）”的缩写。它可以从一组包含“局外点”的观测数据集中，通过迭代方式估计数学模型的参数。它是一种不确定的算法——它有一定的概率得出一个合理的结果；为了提高概率必须提高迭代次数。

RANSAC的基本假设是： 
（1）数据由“局内点”组成，例如：数据的分布可以用一些模型参数来解释； 
（2）“局外点”是不能适应该模型的数据； 
（3）除此之外的数据属于噪声。 
局外点产生的原因有：噪声的极值；错误的测量方法；对数据的错误假设。 

一个简单的例子是从一组观测数据中找出合适的2维直线。假设观测数据中包含局内点和局外点，其中局内点近似的被直线所通过，而局外点远离于直线。简单的最小二乘法不能找到适应于局内点的直线，原因是最小二乘法尽量去适应包括局外点在内的所有点。相反，RANSAC能得出一个仅仅用局内点计算出模型，并且概率还足够高。但是，RANSAC并不能保证结果一定正确，为了保证算法有足够高的合理概率，我们必须小心的选择算法的参数。 

<img src="https://img-blog.csdn.net/20180126214840833?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcm9iaW5oand5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img" style="zoom: 80%;" />

#### 4.4.3 RANSAC原理  
​		OpenCV中滤除误匹配对采用RANSAC算法寻找一个最佳单应性矩阵H，矩阵大小为3×3。RANSAC目的是找到最优的参数矩阵使得满足该矩阵的数据点个数最多，通常令h3=1来归一化矩阵。由于单应性矩阵有8个未知参数，至少需要8个线性方程求解，对应到点位置信息上，一组点对可以列出两个方程，则至少包含4组匹配点对。

![img](https://img-blog.csdn.net/20151208160537989)

​		其中(x,y)表示目标图像角点位置,(x',y')为场景图像角点位置，s为尺度参数。

​		RANSAC算法从匹配数据集中随机抽出4个样本并保证这4个样本之间不共线，计算出单应性矩阵，然后利用这个模型测试所有数据，并计算满足这个模型数据点的个数与投影误差(即代价函数)，若此模型为最优模型，则对应的代价函数最小。

#### 4.4.4 RANSAC算法步骤
1. 随机从数据集中随机抽出4个样本数据 (此4个样本之间不能共线)，计算出单应矩阵H，记为模型M；

2. 计算数据集中所有数据与模型M的投影误差，若误差小于阈值，加入内点集 I ；
   
3. 如果当前内点集 I 元素个数大于最优内点集 I_best , 则更新 I_best = I，同时更新迭代次数k ;
   
4. 如果迭代次数大于k,则退出 ; 否则迭代次数加1，并重复上述步骤；

 注：迭代次数k在不大于最大迭代次数的情况下，是在不断更新而不是固定的，其中，p为置信度，一般取0.995；w为"内点"的比例 ; m为计算模型所需要的最少样本数=4；

## 第五节——基于特征点匹配的图像融合

这里先给出流程图像拼接的过程，我们按过程来一步步复习

1. 对处理后的图像进行特征点提取
2. 对特征点进行匹配
3. 对图像进行透视变换并拼接
4. 对于拼接边沿和整体明暗度进行特殊处理

### 5.1 特征点描述与匹配

对于我们即将要匹配的两张图像，我们需要先利用特征点检测的方法计算出二者的特征点，再将两张图进行匹配，再匹配的过程中，我们应当遵守图像的尺度一致性和角度一致性。

先来说说如何计算出特征点的方向角

#### 5.1.1 计算特征描述子

对于一个感兴趣区域的梯度方向角，计算它分为5步：

1. 图片长宽比标准化
2. 利用一阶微分，计算$(x,y)$方向上的导数
3. 在n * n的cell中计算梯度直方图
4. 标准化梯度向量，使其对于亮度不敏感
5. 计算整个感兴趣区域的HoG向量（方向描述子）

#### 5.1.2 特征匹配

下面简要说明一下如何对两张图片的特征点进行匹配，它同样分为5步：

1. 选出一系列特征点
2. 对每个特征点，建立一个感兴趣区域
3. 提取并归一化这些感兴趣区域
4. 计算特征描述子
5. 匹配特征描述子

特征子匹配的方法如图：

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210707105228060.png" alt="image-20210707105228060" style="zoom:50%;" />

### 5.2 特征描述子

#### 5.2.1 SIFT特征描述子

SIFT算法，中文名又称“尺度不变特征转换”算法，它用来侦测与描述影像中的局部性特征，它在空间尺度 中寻找极值点，并提取出其位置、尺度、旋转不变量，局部影像特征的描述与侦测可以帮助辨识物体， SIFT特征是基于物体上的一些局部外观的兴趣点而与影像的大小和旋转无关。对于光线、噪声、些微视 角改变的容忍度也相当高。基于这些特性，它们是高度显著而且相对容易撷取，在母数庞大的特征数据 库中，很容易辨识物体而且鲜有误认。 

SIFT算法对输入图像构造图像金字塔，再对图像金字塔进行差分运算，从而提取出图像在不同频域下的 极值点，再对与每个极值点确定其极值点方向，最后通过筛选得出该图像的特征描述子。流程如下所示：

1. 创建高斯差分金子塔，便于在不同尺度（频率）上提取特征
2. 计算图像梯度
3. 使用高斯函数卷积每个小区域的梯度
4. 构建SIFT描述符

<img src="C:\Users\DJW74\AppData\Roaming\Typora\typora-user-images\image-20210707112141702.png" alt="image-20210707112141702" style="zoom: 50%;" />

#### 5.2.2 ORB算法 

ORB 全称为Oriented FAST and Rotated BRIEF，是一种快速特征点提取和描述的算法。从名字中，我 们可以看出是由两部分构成，Oriented FAST 和 Rotated BRIEF，这也以最简单的语言描述了ORB算 法。ORB算法分为两部分，分别是特征点提取和特征点描述。特征提取是由FAST算法（前面已经学过） 发展来的，特征点描述是根据BRIEF特征描述算法改进的。 

ORB算法首先通过FAST算法提取特征点，再建立图像金字塔，实现对特征点尺度不变性和旋转不变性的 检测，最终得到每个特征点的特征描述子，ORB算法最大的特点就是计算速度快，计算时间大概只有 SIFT的1%，SURF的10%，这主要是因为使用了FAST来加速了特征点的提取。再次是使用BRIEF算法计算 描述子，该描述子特有的2进制串的表现形式不仅节约了存储空间，而且大大缩短了匹配的时间，但ORB 算法也有其不足之处，比如尺度变换的应对能力比较低。

### 5.3特征匹配

特征的匹配是针对特征描述子的进行的，第一步计算出的特征描述子通常是一个向量，两个特征描述子 的之间的距离可以反应出其相似的程度，也就是这两个特征点是不是同一个。根据描述子的不同，可以 选择不同的距离度量。如果是浮点类型的描述子，可以使用其欧式距离；对于二进制的描述子（BRIEF） 可以使用其汉明距离（两个不同二进制之间的汉明距离指的是两个二进制串不同位的个数）。 

有了计算描述子相似度的方法，那么在特征点的集合中如何寻找和其最相似的特征点，这就是特征点的 匹配了。 

1. 暴力匹配方法(Brute-Froce Matcher) ：计算某一个特征点描述子与其他所有特征点描述子之间的距 离，然后将得到的距离进行排序，取距离最近的一个作为匹配点。
2.  过滤错误匹配方法： 汉明距离小于最小距离的两倍 ：选择已经匹配的点对的汉明距离小于最小距离的两倍作为判断依据，如 果小于该值则认为是一个错误的匹配，过滤掉；大于该值则认为是一个正确的匹配。 
3. 交叉匹配 ：交叉过滤的是想很简单，再进行一次匹配，反过来使用被匹配到的点进行匹配，如果匹配到 的仍然是第一次匹配的点的话，就认为这是一个正确的匹配。举例来说就是，假如第一次特征点A使用 暴力匹配的方法，匹配到的特征点是特征点B；反过来，使用特征点B进行匹配，如果匹配到的仍然是特 征点A，则就认为这是一个正确的匹配，否则就是一个错误的匹配。
4.  KNN匹配 ：K近邻匹配，在匹配的时候选择K个和特征点最相似的点，如果这K个点之间的区别足够大， 则选择最相似的那个点作为匹配点，通常选择K = 2，也就是最近邻匹配。对每个匹配返回两个最近邻的 匹配，如果第一匹配和第二匹配距离比率足够大（向量距离足够远），则认为这是一个正确的匹配，比 率的阈值通常在2左右。 
5. RANSAC（随机采样一致性）： 该方法利用匹配点计算两个图像之间单应矩阵 ，然后利用重投影误差来 判定某一个匹配是不是正确的匹配。

## 第六节 相机（二）

### 6.1 立体相机概述

（蚌埠不住了，写完这个睡觉）

我们对双眼的立体成像能力非常熟悉，但是在计算机软件中，我们可以模仿这种能力到什么程度呢？实际上，计算机是通过寻找两个成像仪上的对应点来完成这个任务。其原理是：**查找某个点在两张图片的对应点，通过其和摄像机基线之间的距离进行计算，可以得到这个点的三维位置。**尽管两张图片的对应点的搜索计算量比较高，但我们可以利用几何知识，限定搜索的范围，降低计算量。因此，使用双目相机进行立体成像包括以下4个步骤：

1. **去畸变**：使用数学方法消除径向和切向的畸变
2. **机位标定**：调整两个相机的角度和其之间的距离，保证两个相机输出的是帧行对齐的图像，尽可能保证两个图像是共面的。
3. **匹配**：在左右相机图像中找到相同的特征，此步骤可以输出视差图，视差是指**左右摄像机观察得到的相同特征在x方向上的差值：** xl−xrxl−xr
4. **重投影**：如果相机的位置关系已知，则可以通过三角测量法将视差图像转换成距离，这一步骤称之为**重投影**，输出为一个深度图

假如我们有一个这样的理想装置：一个完美的无畸变、已对准、已测量好的系统，两个摄像机的图像平面彼此完全共面，具有完全平行的光轴（从投影中心O发出，通过主点c的一条射线），他们之间的距离是已知的T，并且具有相等的焦距：$f_l=f_r$，且假设主点 $c_{leftx},c_{rightx}$ 已经被校准为在其各自的图像中具有相同的像素坐标。

<img src="https://robot.czxy.com/docs/camera/chapter03/assets/image-20200403161101821.png" alt="image-20200403161101821" style="zoom:67%;" />

假设我们在物理世界中找到一个点PP，在左右图像上的成像点为$p_l$和$p_r$，对应的横坐标为$x_l$和$x_r$，在此种简化的情况下，我们可以发现**深度$Z$和视差 $x_l−x_r$ 成反比**。即越近的图像，视差越大。

$$
\frac{T−(xl−xr)}{Z−f}=\frac{T}Z⇒Z=\frac{f.T}{x_l−x_r}
$$

由于深度与视差成反比，且显然，其之间存在非线性的关系，当视差趋近于零时，微小的视差变换会造成很大的深度变换。且当视差较大时，微小的视差变化不会对深度有太大的改变。则可得出以下结论：**双目立体视觉系统只有在物体和摄像机距离较近时，才有比较高的深度分辨率。**如下图所示：

<img src="https://robot.czxy.com/docs/camera/chapter03/assets/image-20200403164042854.png" alt="image-20200403164042854" style="zoom:50%;" />

- 下图为OpenCV立体视觉坐标系

用于已进行去畸变的相机，图像左上角为像素坐标系原点，两个图像平面的行与行对齐，整个双目摄像机的坐标系以左目相机的投影中心为原点，采用右手坐标系法则。

<img src="https://robot.czxy.com/docs/camera/chapter03/assets/image-20200403164130848.png" alt="image-20200403164130848" style="zoom:50%;" />

由于立体视觉，我们可以估计任何物体的深度（假设我们已经做了正确的矩阵校准），它可以计算一个深度图或视差图:

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200924155210613.png#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200924155253457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NqaF9zamhfc2po,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200924155315267.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NqaF9zamhfc2po,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200924155608610.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NqaF9zamhfc2po,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020092415573147.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NqaF9zamhfc2po,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200924160053816.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NqaF9zamhfc2po,size_16,color_FFFFFF,t_70#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200924160257954.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NqaF9zamhfc2po,size_16,color_FFFFFF,t_70#pic_center)

根据前面的原理介绍，我们总结一下基于双目立体视觉法深度相机的优缺点;
1.优点
1、对相机硬件要求低，成本也低。因为不需要像TOF和结构光那样使用特殊的发射器和接收器，使用普通的消费级RGB相机即可。

2、室内外都适用。由于直接根据环境光采集图像，所以在室内、室外都能使用。相比之下，TOF和结构光基本只能在室内使用。

2.缺点
1、对环境光照非常敏感。双目立体视觉法依赖环境中的自然光线采集图像，而由于光照角度变化、光照强度变化等环境因素的影响，拍摄的两张图片亮度差别会比较大，这会对匹配算法提出很大的挑战。如下图是在不同光照条件下拍摄的图片：

另外，在光照较强（会出现过度曝光）和较暗的情况下也会导致算法效果急剧下降。

2、不适用于单调缺乏纹理的场景。由于双目立体视觉法根据视觉特征进行图像匹配，所以对于缺乏视觉特征的场景（如天空、白墙、沙漠等）会出现匹配困难，导致匹配误差较大甚至匹配失败。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200924172849959.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NqaF9zamhfc2po,size_16,color_FFFFFF,t_70#pic_center)

3、计算复杂度高。该方法是纯视觉的方法，需要逐像素计算匹配；又因为上述多种因素的影响，需要保证匹配结果比较鲁棒，所以算法中会增加大量的错误剔除策略，因此对算法要求较高，想要实现可靠商用难度大，计算量较大。

4、相机基线限制了测量范围。测量范围和基线（两个摄像头间距）关系很大：基线越大，测量范围越远；基线越小，测量范围越近。所以基线在一定程度上限制了该深度相机的测量范围。

